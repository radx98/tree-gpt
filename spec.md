# App Spec

## 1. Core concept

The app is a ChatGPT-like interface where conversations are **non-linear** and visualized in **columns** as a 2D branching tree.

* **Vertical axis**: time within a single “chat block” (a self-contained thread).
* **Horizontal axis**: branching depth.

  * When the user asks about a specific fragment of text, a **new chat block** is created in a new (or existing) column to the right, visually linked to that fragment.

The **initial chat in the first column** is just a special case of a **chat block**. Every block, including the first, behaves the same: it holds a linear conversation between the user and the LLM, can have branches, and has an AI-generated header.

---

## 2. Terminology

* **Column**
  Vertical strip of content at a given horizontal depth (0 = root). A column can contain multiple chat blocks.

* **Chat block**
  A self-contained linear conversation thread:

  * Contains an **AI-generated header**.
  * Contains a sequence of user prompts and LLM responses.
  * Has its own **linear input** at the bottom (for continuing that thread).
  * Is visually anchored to a **source selection** in the column to its left (except the first/root block, which has no source).

* **Message**
  Individual user prompt or LLM response within a chat block.

* **Linear input field**
  The input at the bottom of a chat block used to continue that thread.

* **Branch input field**
  A temporary input that appears **above a text selection**; sending it creates a **new chat block** to the right.

---

## 3. Layout & appearance

### 3.1 Columns

* Columns are laid out horizontally from left to right.

* A column is created at depth **k+1** when a chat block is created branching from a block in column **k** (if column k+1 doesn’t already exist).

* **Width**: each column has a max width of approximately `max-w-2xl`.

  * On narrow viewports, columns shrink to fit while maintaining a consistent, readable width.

* At any given time, **one column is “current”**:

  * It is centered in the viewport.
  * It is visually emphasized (e.g. slight shadow, higher opacity).

### 3.2 Chat block structure

Each chat block includes:

1. **Header**

   * Short title phrase generated by the AI based on the content of the block (e.g. “Clarifying matrix multiplication”, “Example for clause X”).
   * Displayed at the top of the block as its label.
   * For the first/root chat block, its header is also used as the session name, and the session title is always kept identical to this header.
   * Until the first LLM response arrives, the header may be empty; the UI can show a neutral placeholder such as “New thread” or a loading indicator.

2. **Messages area**

   * A vertical sequence of:

     * User prompts
     * LLM responses
     * Optional system/UI elements (e.g. loading indicator, error messages) that are rendered purely in the UI and are not stored as messages.

3. **Linear input**

   * At the bottom of the block, directly under the last LLM response that has no user reply yet.
   * Used to extend the conversation linearly within that block.

4. **Visual container**

   * The whole block has a subtle border or card-like background, so multiple blocks in the same column are clearly distinct.
   * The block can be in expanded or collapsed state (see §6).

### 3.3 Message styling

Within each chat block:

* **User prompts**

  * Horizontally aligned to the **right**.
  * Occupy **90%** of the column width.
  * Have a **soft blue background** (bubble/card style).
  * Include timestamp or small metadata as needed (optional visual detail).

* **LLM responses**

  * Horizontally aligned to the **left**.
  * Occupy **90%** of the column width.
  * Neutral/light background, distinct from user bubbles.

* Messages stack vertically in chronological order:

  * User → LLM → User → LLM → …

### 3.4 Linear input field

For each chat block:

* There is exactly one linear input associated with that block:

  * It is rendered under the last LLM response that does not yet have a user message under it.
  * In total, there is at most one linear input per block, but only inputs that belong to the current column are active.

* Styling:

  * Uses the same bubble/card style as a user prompt (aligned right, 90% width, soft blue container).
  * Inside that bubble is a white text input (textarea).

* Behavior:

  * On send (Enter / Send button):

    * The text is treated as the new user prompt for that block.
    * A request is sent to the LLM (see §§11–12).
    * When the LLM response arrives, a user message and an assistant message are appended to the block’s messages; a new linear input then appears under the new assistant message.

* Linear inputs in non-current columns are still visibly rendered to preserve layout, but they are inactive: they cannot be focused or typed into until their column becomes current.

This logic is identical for:

* The **initial chat block** in the first column.
* Every **child chat block** in any column.

---

## 4. Navigation & column behavior

### 4.1 Current column

* Exactly one column at a time is considered current and is centered.
* Interactions such as typing into linear inputs, keyboard shortcuts, and primary scrolling focus happen in this column, but text selection can occur in any visible column.
* Only the linear inputs in the current column accept focus and keyboard input; linear inputs belonging to blocks in other columns are visually present but inactive until those columns become current.

### 4.2 Horizontal navigation

* **Desktop:**

  * **Floating left/right arrow buttons** placed immediately left and right of the current column.

    * Clicking left moves focus to the previous column (if any).
    * Clicking right moves focus to the next column (if any).

* **Mobile:**

  * **Swipe left/right** horizontally to move between columns, like a carousel.

* When moving:

  * The new column slides into the center.
  * The arrows/swipe targets update (no left arrow if you’re on the leftmost column, etc.).

---

## 5. Branching interactions

### 5.1 Selecting text

* The user can click-and-drag to select **any text** inside any message (user or LLM) in any column.
* Once there is a non-empty selection:

  * The selected text is visually **highlighted** (e.g. slightly tinted background).
  * A **branch UI affordance** appears near the selection:

    * The **branch input field**.

### 5.2 Branch input field

* The branch input field appears **above or near the selection**, anchored to its vertical position.
* It is a small text input (or compact bubble) that lets the user **ask a question specifically about the highlighted fragment**.
* Behavior:

  * User types a prompt and presses Enter or clicks “Send”.
  * On send:

    * The branch input field disappears.
    * A new **chat block** is created in the **column to the right** of the column where the selection lives.

### 5.3 Creating a new chat block from a selection

Given a selection in column k:

1. Determine child column:

   * If column k+1 does not exist yet, create it.
   * If it exists, a new chat block is added to that column.

2. Create a chat block at aligned height:

   * Compute the vertical position of the highlighted text in column k.
   * In column k+1, create a new chat block whose initial anchor position aligns vertically with that selection.
   * Draw a connector line from the selection in column k to the new chat block’s container in column k+1.

3. Populate the new chat block:

   * Use the LLM’s block_header field (see §11.6) as the AI-generated header summarizing the new branch, based on the selection and the user’s branch question.
   * Inside the block:

     * Treat the branch prompt as the new user input for this block.
     * Request and then render the LLM’s response as the first assistant message.
     * Render a linear input under that response for continuing this branch.

4. Persist highlight & link:

   * The selection in the parent message remains highlighted.
   * The connector line persists, visually affiliating this chat block with its source text.

### 5.4 Branching from deeper columns

* The same mechanism works recursively:

  * A selection in column **k** produces a chat block in column **k+1**.
  * That chat block can itself contain messages that can be selected to spawn blocks in column **k+2**, and so on.

---

## 6. Multiple chat blocks per column & collision handling

### 6.1 Multiple chat blocks

* Any column can have **multiple chat blocks**:

  * e.g., multiple selections in column 1 can each spawn their own chat blocks in column 2.
  * All of these blocks live in column 2 at different vertical positions, each linked back to their respective selections.

### 6.2 Initial positioning

* For each new chat block in column **k**:

  * Its **initial vertical position** is aligned so that the top of the block is at the same height as its **source selection** in column **k-1**.

### 6.3 Overlap detection (“too close”)

* After positioning, the system checks for **vertical overlap** between chat blocks in the same column:

  * If two or more blocks intersect or are within a defined minimal gap, they are considered **too close**.

### 6.4 Collapse & expand behavior

When overlap is detected:

1. The overlapping chat blocks (the ones in that “crowded” vertical region) switch to **collapsed state**:

   * A collapsed block shows a compact summary:

     * The header.
     * Possibly a short snippet or icon indicating it contains a conversation.
   * The full message history and linear input are hidden.

2. Their positions are **adjusted**:

   * The blocks are moved so they no longer overlap.
   * The displacement is **distributed symmetrically around their initial anchor positions**:

     * For two blocks: one nudged slightly up, the other slightly down.
     * For more than two: they’re spaced out vertically around the anchor area.
   * Connector lines remain intact and point from each block to its source selection.

3. Expansion:

   * A collapsed block can be **manually expanded** by the user (e.g., click on header).
   * Expanding one block may:

     * Push nearby blocks further to make room, or
     * Temporarily collapse them, depending on available space.
   * In expanded state, all messages and the linear input area are visible again.


---

## 7. Session header & dropdown

* Every chat block has a header, but the first/root chat block’s header is special:

  * It becomes the session name.
  * The session title is always kept identical to the root chat block’s header. Whenever the root header is set or updated, the session title is set to the same value; the two never diverge.

* A floating button in the top-left corner of the viewport opens a dropdown menu:

  * The dropdown displays the current session’s name, which is derived directly from the root block’s header.
  * It can later be extended to list other sessions or conversation metadata (not required to implement the branching logic itself).

* There’s the app name to the right of the button.

---

## 8. Behavioral summary

Putting it all together:

1. The user starts a new session:

    * A first chat block appears in column 0 with an empty linear input.
    * They type a prompt and send it. The text is treated as the new user input for that block and sent to the LLM.
    * Once the LLM responds, the prompt and reply appear together in the block as a right-aligned soft-blue user message followed by a left-aligned assistant message, and a new linear input appears below the reply for the next turn.

2. They continue linearly in that block as in a normal chat.

3. At any time, they can select specific text in any message:

   * A branch input appears above the selection.
   * They type a question and send it.
   * A new chat block appears in the column to the right, vertically aligned to the selection, with a connector line linking them.

4. That new chat block behaves like a normal chat: linear input, multiple turns, etc.

5. If multiple chat blocks in a column are created near each other:

   * They are collapsed and repositioned to avoid overlap, and can be expanded on demand.

6. The user navigates between columns via floating left/right arrows (desktop) or horizontal swipe (mobile).
   One column is always centered and considered current.

7. The header of the first chat block is AI-generated and used as the session name, visible in a dropdown from the floating button in the top-left.

---

## 9. Data model and identifiers

The app treats everything as JSON objects in memory that can be serialized into browser storage and sent to the LLM.

### 9.1 Session

Each conversation session is represented as a single JSON object containing:

* A unique session id, for example `"session_123"`.
* A human-readable session title, which is the header of the root chat block, for example `"Clarifying matrix multiplication"`.
* A reference to the root chat block id, for example `"block_root"`.
* A collection of all chat blocks in this session.
* Optional metadata such as creation time and last updated time.

Conceptually, a minimal session object looks like this:

{ "id": "session_123", "title": "Clarifying matrix multiplication", "rootBlockId": "block_root", "blocks": { ... }, "createdAt": "ISO timestamp", "updatedAt": "ISO timestamp" }

The `blocks` field contains all chat blocks by id, not grouped by columns. Columns are derived from each block’s depth.

### 9.2 Chat block

Each chat block is represented as a JSON object keyed by its id inside session.blocks. It contains:

* A unique block id, for example "block_7".
* A depth integer (0 for the root column, 1 for the first branch column, etc.).
* The AI-generated header string for that block, or null if the header has not yet been set.
* A messages array (linear history of user and LLM messages in this block).
* A source object describing where this block branched from (or null for the root).
* UI-related flags such as collapsed (true/false).

A typical chat block shape:

{
"id": "block_7",
"depth": 2,
"header": "Concrete example of clause X",
"source": {
"parentBlockId": "block_3",
"parentMessageId": "msg_18",
"selection": {
"text": "string of highlighted text",
"startOffset": 120,
"endOffset": 180
}
},
"messages": [ ... ],
"collapsed": false
}

The source object allows the UI to reconstruct connector lines and vertical alignment without storing actual pixel positions. The layout engine uses parentBlockId, parentMessageId, and the selection information to compute the anchor point; the full content of the parent message is read from the parent block’s messages array when needed.

The root chat block has source set to null and depth set to 0.

### 9.3 Message

Each message inside a chat block is represented as a JSON object with:

* A unique message id within the session, for example `"msg_5"`.
* A `role` field: `"user"` or `"assistant"`.
* The `text` content of the message.
* Timestamps and optional metadata such as streaming status or error flags.

A message object looks like:

{ "id": "msg_5", "role": "user", "text": "What happens if the matrix is singular?", "createdAt": "ISO timestamp" }

Messages are stored in chronological order in `block.messages`:

[
{ "id": "msg_1", "role": "user", ... },
{ "id": "msg_2", "role": "assistant", ... },
{ "id": "msg_3", "role": "user", ... },
...
]

This is an append-only log inside each block: new user messages and new assistant messages are always appended at the end.

System/UI-only elements such as loading indicators or transient error messages are not stored as messages; they are rendered purely from ephemeral UI state.

### 9.4 Selection and branching links

Selections themselves are not stored as separate top-level objects. Instead, when a branch is created:

* The app records a transient selection (message id + character range + selected text).
* When the branch is actually created (user sends branch prompt), this information is copied into the new block’s `source` field.

The `source` object therefore permanently links:

* The new block id.
* The parent block and parent message id.
* The exact selected text.
* The full content of the parent message where the selection was made.

This makes it possible to reconstruct the tree structure and the context of each branch purely from JSON, without needing UI-specific state.

### 9.5 Column derivation

Columns are not stored as separate entities. Instead:

* The `depth` number on each block determines which column it belongs to.
* All blocks with `depth = 0` are in the leftmost column, with `depth = 1` in the next column, and so on.

Within a column, the vertical ordering is a rendering concern:

* The layout engine can compute an initial vertical anchor based on the parent selection height.
* Overlap resolution and the “too close → collapse” logic rely on run-time layout, not stored data.

Optionally, the app may store a rough `anchorOrder` integer per block for more stable vertical ordering across reloads, for example:

{ "anchorOrder": 3 }

This value only encodes relative order (1st, 2nd, …) in that column rather than pixel positions.

### 9.6 UI state vs persistent state

Some state is ephemeral and not stored in persistent JSON:

* Current text in the linear input field.
* Current text in a branch input field.
* Currently selected text range in the UI before a branch is created.
* Scroll positions and focus.

Some UI state is useful to persist:

* Whether a block is collapsed or expanded: `collapsed: true` or `false`.
* The id of the currently active session.
* Optionally the id of the currently focused block or column.

A top-level app state JSON may therefore look like:

{
"version": 1,
"activeSessionId": "session_123",
"sessions": {
"session_123": { ... },
"session_456": { ... }
},
"ui": {
"lastFocusedBlockId": "block_7"
}
}

Only this JSON is written to browser storage; everything else (inputs, temporary selections) lives just in memory.

---

## 10. Local browser storage

All persistent state is stored as a single JSON document in local browser storage (for example under a key such as `"branching_chat_state"`).

### 10.1 Initial load

On app startup:

1. The app reads the JSON string from browser storage.
2. If nothing is stored, it initializes an empty state such as:
   { "version": 1, "activeSessionId": null, "sessions": {} }.
3. If parsing fails or the structure is invalid, the app falls back to this empty state and may keep a copy of the broken data under a separate key for manual recovery if needed.

### 10.2 Saving on changes

Whenever a meaningful change is made, the in-memory JSON state is updated and then serialized back into browser storage. Meaningful changes include:

* Creating a new session and its root block.
* Adding a new user message.
* Storing a new assistant message from the LLM.
* Creating a new branch (new block with `source`).
* Updating a block’s header.
* Toggling a block’s `collapsed` flag.
* Changing the session title.
* Switching the active session.

To keep the implementation simple but not wasteful, the app can:

* Update the in-memory JSON immediately.
* Schedule a single write to browser storage after a short delay (for example, a few hundred milliseconds), coalescing multiple rapid changes into one write.

From the data perspective, this means browser storage always contains the latest known consistent snapshot of:

{ "version": 1, "activeSessionId": "...", "sessions": { ... }, "ui": { ... } }

### 10.3 Session lifecycle

When a new session is created (for example, when the user clicks “New chat” or otherwise starts a new conversation), the app proactively creates its data structures before the first message is sent:

* A new session object is added to state.sessions.
* A root block with depth: 0 and source: null is created and attached to it.
* activeSessionId is updated to point to the new session.
* The state is persisted.

When a user later sends the first prompt in that session, it is handled as described in §§11–12 using this already-existing root block.

When sessions are closed or deleted (if supported):

* The corresponding session entry is removed from state.sessions.
* If the deleted session was active, activeSessionId is reset or switched to another session.
* The state is persisted.

### 10.4 Versioning

The top-level JSON includes a simple numeric `version` field. If future changes require migrations, the app can:

* Read `version`.
* Transform the stored data into the current shape.
* Save back with the new `version`.

The current description assumes `version = 1` as the initial format.

---

## 11. LLM interaction model

All interactions with the LLM (OpenAI API) use a single, universal JSON structure that can handle:

* Normal chat replies inside any chat block.
* Generation of AI headers for chat blocks.
* Generation of the root block header that becomes the session title.

The OpenAI Chat Completions API is used with:

* A system message that explains the app’s expectations.
* A single user message whose content is a JSON object describing the current request.

### 11.1 Universal request JSON

The content of the user message is always a JSON object containing at least:

* `"request_type"`: for example `"chat_block_turn"`.
* `"session"`: minimal session info relevant to the current request.
* `"branch_path"`: full context of the current branch from root to the current block.
* `"current_user_input"`: the text the human just typed.
* `"options"`: optional flags and parameters, extendable over time.

The options field is an extensible object for additional flags and parameters, for example:

* should_suggest_block_header: whether the model should propose or refine a short header for the current block.
* should_suggest_session_title: a hint that, for the root block, the suggested block header will also be used as the session title, so it should be especially concise and descriptive.
* language: preferred natural language for the assistant_message and headers.

These flags influence how the model chooses block_header, but the app still enforces that the session title is always identical to the root block’s header.


A typical request payload looks like:

{
"request_type": "chat_block_turn",
"session": {
"id": "session_123",
"title": "Clarifying matrix multiplication"
},
"branch_path": [ ... ],
"current_user_input": "user prompt string",
"options": {
"should_suggest_block_header": true,
"should_suggest_session_title": true or false,
"language": "en"
}
}

The `branch_path` gives the LLM the full context of the branch.

### 11.2 Branch path context

The branch_path array is ordered from the root block down to the current block. Each element represents one block along the path and contains:

* The block id.
* The block header (if already known).
* The full linear message history inside that block up to, but not including, the new user input that triggered this request.
* For non-root blocks, a source object describing how this block branched from its parent.

Each element in branch_path therefore has a structure like:

{
"block_id": "block_3",
"header": "Clarifying step 2",
"source": {
"parentBlockId": "block_root",
"parentMessageId": "msg_7",
"selection": {
"text": "selected snippet",
"startOffset": 50,
"endOffset": 120
}
},
"messages": [
{ "role": "user", "text": "..." },
{ "role": "assistant", "text": "..." },
...
]
}

For the root block element:

* source is null.
* depth is implicitly 0 and does not need to be sent if the model does not need it.

The last element in branch_path corresponds to the chat block the LLM is currently replying inside. Its messages array contains all previous turns in that block, but never the new user input that is currently being sent; that input is provided only in current_user_input.

Because the parent messages are included in the messages arrays of the appropriate blocks, the LLM sees both the exact snippet that was selected (via selection) and the full parent message text that snippet came from, without needing a separate fullParentMessageText field.

The LLM therefore sees:

* All parent blocks and how they relate via selections.
* The exact text that was highlighted at each branching step and its character offsets.
* The full messages containing each snippet in the parent blocks.
* All messages in the current block up to the point where the new user input is sent.

This satisfies the requirement that the LLM “gets the full context of the branch.”

The messages objects included in branch_path are a simplified view of the internal messages (role and text only); ids and timestamps are kept in the client state but are not sent to the LLM.

### 11.3 Root block: first reply

When the very first message in a new session is sent:

* A new session and root block are created.
* The root block’s messages array is initially empty.
* The branch_path contains a single element describing this root block, with a messages array containing all previously committed messages (typically none on the first turn).
* current_user_input is set to the text the user just typed.
* options.should_suggest_block_header is set to true.
* options.should_suggest_session_title is set to true as well, because the root block header will become the session title.

Example request:

{
"request_type": "chat_block_turn",
"session": {
"id": "session_999",
"title": null
},
"branch_path": [
{
"block_id": "block_root",
"header": null,
"source": null,
"messages": []
}
],
"current_user_input": "Explain matrix multiplication in simple terms.",
"options": {
"should_suggest_block_header": true,
"should_suggest_session_title": true
}
}

### 11.4 Continuing a chat block

When adding another message to an existing chat block (no new branch):

* The block already has a non-null header.
* The user types a prompt in the linear input for that block and presses send.
* For this request:

  * The block’s existing messages array is treated as the history before the new turn.
  * branch_path is built by walking from the root block to the current block via source.parentBlockId, including the full messages arrays for each block up to but not including the new user input.
  * current_user_input is set to the text the user just typed.
  * options.should_suggest_session_title can be false (no need to rename the session).
  * options.should_suggest_block_header can be true if header suggestions are still desired, or false if the header should remain stable.

When the LLM response arrives, the app appends two messages to the current block’s messages array in order:

1. A user message containing the text from current_user_input.
2. An assistant message containing the assistant_message text.

The header is only updated if the block currently has no header or if the UI explicitly allows header refresh.

### 11.5 Creating a branch

When the user selects text and sends a branch prompt:

1. The app records the selection: parent block id, parent message id, selected text and offsets.
2. A new block object is created with the appropriate depth, and its source field is filled with this selection information.
3. For the purposes of the LLM request, this new block initially has an empty messages array.
4. branch_path is built from the root block all the way to this new block:

   * For each ancestor block (including the parent), messages contains the full message history in that block up to this point.
   * The new block appears as the last element in branch_path with an empty messages array and the appropriate source.
5. current_user_input is set to the branch prompt text that the user typed in the branch input.

The LLM therefore knows:

* The overall conversation that led to the parent block.
* The exact text that was highlighted (via selection) and its character offsets.
* The full message where the highlight lives (from the parent block’s messages).
* The prompt used to start the new branch (current_user_input).

When the response arrives, the app appends two messages to the new block’s messages array:

1. A user message containing the branch prompt text (current_user_input).
2. An assistant message containing the assistant_message text.

It then sets the new block’s header from block_header.

### 11.6 LLM response JSON

The LLM is instructed (via the system message) to always respond with a single JSON object containing at least:

* "assistant_message": the text that should be displayed as the assistant’s reply in the current block.
* "block_header": an optional short header summarizing the block’s topic.
* "session_title": an optional session title suggestion. For the root block, if this field is present it must be identical to block_header; the app treats the root block’s header as the single source of truth and always keeps the session title equal to that header.
* "notes": an optional diagnostic or explanation field that is ignored by the UI.

A typical response:

{
"assistant_message": "Explanation of the concept...",
"block_header": "Intuitive explanation of matrix multiplication",
"session_title": null,
"notes": "short internal comment or can be null"
}

On receiving this JSON:

1. The app appends an assistant message to the current block’s messages array using the value of assistant_message, after first appending a user message based on current_user_input as described in §§11.3–11.5.
2. If block_header is non-empty and the block currently has no header, the block’s header field is set to this value. If the block already has a header and header refresh is not enabled, block_header is ignored.
3. If the block is the root block and block_header is non-empty, the session’s title is always set to exactly the same value as the root block’s header. Any separate session_title value in the response is either null or identical to block_header; the app never allows the session title to diverge from the root header.

This makes header generation part of the normal reply path and avoids separate special-purpose calls while enforcing that the root header and session title are always the same.

### 11.7 Error handling

If the LLM request fails (network error, parsing error, invalid JSON, etc.):

* A synthetic message may be added to the block with `role: "assistant"` and a short error explanation, for example `"text": "Something went wrong. Please try again."`, or the error can be surfaced via a non-persistent UI element.
* The failed attempt is not persisted as a real assistant message unless explicitly desired.
* The user can retry; the same `branch_path` and `current_user_input` are used to build a new request.

If the LLM returns JSON that cannot be parsed:

* The raw text is stored in a special error field in the app state for debugging (optional).
* A user-facing error is displayed.
* No new assistant message is appended until a valid structure is received.

---

## 12. Data flow for common actions

### 12.1 Sending a prompt in the root block

1. The user starts a new session, which creates a session object and a root block (see §10.3). A first chat block appears in column 0 with an empty linear input.

2. The user types a prompt in the root block’s linear input and presses send.

3. The app:

   * Uses the root block’s current messages array as the history (typically empty for the very first turn).
   * Builds branch_path with only the root block, including its existing messages.
   * Sets current_user_input to the text the user just typed.
   * Sends the universal JSON request to the LLM.

4. When the response JSON arrives, the app:

   * Appends a user message to rootBlock.messages containing the text from current_user_input.
   * Appends an assistant message containing assistant_message.
   * Sets the block header from block_header if provided.
   * Sets the session title to exactly the same value as the root block’s header.
   * Writes the updated session into browser storage.

### 12.2 Sending a prompt in any existing block

1. The user types in the block’s linear input and presses send.

2. The app:

   * Takes that block’s existing messages array as the history.
   * Builds branch_path from the root block to this block via source.parentBlockId, including the full messages arrays for each block.
   * Sets current_user_input to the text the user just typed.
   * Sends the universal JSON request.

3. When the response arrives, the app:

   * Appends a user message to that block’s messages containing current_user_input.
   * Appends an assistant message containing assistant_message.
   * Updates header only if the block currently has no header or if the UI explicitly allows header refresh.
   * Persists the state to browser storage.

### 12.3 Creating a branch

1. The user selects text in a message in block A.

2. A branch input appears above the selection.

3. The user types a branch prompt and sends it.

4. The app:

   * Creates a new block B with depth = depth(A) + 1.
   * Fills block B’s source with parent block id, parent message id, and selection text and offsets.
   * Builds branch_path from the root block through A and finally to B:

     * For all existing blocks along the path (including A), branch_path contains their full messages arrays.
     * For the new block B, branch_path includes an element with an empty messages array and the appropriate source.
   * Sets current_user_input to the branch prompt text.
   * Sends the universal JSON request.

5. When the response arrives, the app:

   * Appends a user message with the branch prompt to block B’s messages.
   * Appends an assistant message containing assistant_message to block B’s messages.
   * Sets B’s header based on block_header.
   * Persists the updated session.

---

## 13. Simplicity and extensibility

To keep the implementation simple without sacrificing correctness:

* There is a single canonical JSON structure in memory, mirroring what is stored in browser storage.
* All structural entities (sessions, blocks, messages, branch sources) are plain JSON objects.
* Columns are derived from `depth` instead of stored as a separate layer of data.
* Every LLM call uses the same universal request JSON shape and expects the same universal response JSON shape.
* The branch context is explicit and self-contained: for any LLM call, the LLM can reconstruct the entire path from the root block down to the current block, including all parent messages and selections.

At the same time, the structure is flexible:

* New fields can be added to `options`, `messages`, `blocks`, or `sessions` without breaking existing logic.
* If context window becomes a concern, the `branch_path` structure can be extended later to include summaries or truncated histories, while preserving the same overall shape.
* Additional features such as suggested follow-up questions, tags, or annotations can be added via extra fields in the LLM response JSON (for example `"suggested_questions": [ ... ]`) and stored alongside existing data.

This data model and request/response flow ensure that:

* The UI behavior described in the spec (columns, blocks, branching, collapse/expand, and headers) can be fully reconstructed from JSON.
* The LLM always receives the full relevant context of a branch: what it is replying to, how that content was selected, and all parent blocks and messages along the way.

---

## 14. Technology stack

The app is implemented as a client-heavy Next.js application with a very small server surface for LLM calls. All persistent conversation data lives in the browser.

---

### 14.1 Framework and language

* Next.js with the App Router and TypeScript is used as the main framework.
* React is used for all interactive UI:

  * Column layout.
  * Chat blocks and messages.
  * Branching UI, selection handling, collapse/expand controls.
* The initial page shell (layout, basic HTML) is rendered by Next.js.
  All conversation state is managed on the client and hydrated from browser storage.

---

### 14.2 Styling and UI components

* Tailwind CSS is used for layout and visual design:

  * Horizontal columns, vertical stacking of blocks.
  * Message bubbles (right-aligned user, left-aligned assistant).
  * Card-like containers for chat blocks, floating controls, dropdowns.
* A small headless or utility-first component set is used for primitives such as:

  * Buttons, cards, dropdown menu for the session name, modals if needed.
* Lucide icons are used for:

  * Left/right navigation arrows.
  * Branch indicators.
  * Collapse/expand icons and other small affordances.

---

### 14.3 Client state and browser persistence

* A single global state store holds the entire JSON data model:

  * All sessions, chat blocks, messages and branching links.
  * The id of the active session.
  * Lightweight UI state such as which block is collapsed or focused.
* The store can be implemented with:

  * React context plus a reducer, or
  * A small state library such as Zustand.
* On startup:

  * The store attempts to load the JSON snapshot from localStorage (for example under one key such as branching_chat_state).
  * If not found or invalid, an empty default state is created.
* On every meaningful change (new message, new block, header update, session change):

  * The in-memory state is updated.
  * A debounced write serializes the full state back into localStorage, replacing the previous snapshot.

No external database is required; all persistent data is stored in the browser.

---

### 14.4 LLM integration (OpenAI)

* All OpenAI access happens on the server side through a single Next.js route (for example at /api/chat).
* The client sends a POST request to this route containing:

  * The universal JSON payload described in the data spec (session info, full branch_path, current_user_input, options).
* The server route:

  * Reads the OpenAI API key from environment variables.
  * Calls the OpenAI Chat Completions API with:

    * One system message describing the app’s role and required JSON output shape.
    * One user message whose content is exactly the JSON payload (as text).
  * Parses the model’s response as JSON with fields such as assistant_message, block_header and session_title.
  * Returns a compact JSON response to the client with these values, or an error object if something goes wrong.

The client never handles the API key and only communicates with the Next.js API route.

---

### 14.5 Data flow between UI, storage and LLM

For any user action that sends a prompt (linear or branch):

1. The user submits text via a linear or branch input in a given block.
2. The client store:

    * Updates the session structure if needed (for example, creating a new block for a branch).
    * Does not yet append a new user message to that block’s messages array; the new text is treated as in-flight input and is only added to messages once a valid assistant reply is received.
3. A request payload is derived from the current store:

   * session object.
   * full branch_path from root to the current block, including all parent messages and selections.
   * current_user_input and options.
4. The client sends this JSON payload to the Next.js /api/chat route.
5. When the response arrives:

   * The store appends a new user message to the current block’s messages containing current_user_input.
   * The store appends a new assistant message to the same block containing assistant_message.
   * The block header and session title are updated if new values are provided, following the invariant that the root block’s header and the session title are always identical.
6. The updated state is written to localStorage on the next debounced save.

The UI always renders directly from the single in-memory store; localStorage is just a persistent snapshot of that store.

---

### 14.6 Additional enhancements

The core stack remains minimal, but several additional tools fit well without changing the overall architecture:

* React Query (or a similar library) can manage the lifecycle of LLM calls:

  * Loading and error states per block.
  * Retry behavior for failed calls.
* A markdown renderer can be used if assistant messages should support basic formatting, lists and code-style snippets.
* A small utility layer for:

  * Generating unique ids for sessions, blocks and messages.
  * Normalizing timestamps and other metadata.
* Convenience features such as keyboard shortcuts, quick-jump between columns or search over headers can be added on top of the same JSON state model and do not require changing the persistence or LLM integration layers.